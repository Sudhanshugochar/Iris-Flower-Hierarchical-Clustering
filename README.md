# ğŸ§ª Project 2: Hierarchical Clustering (Step-by-Step)

## ğŸ“Œ Project Overview

This project demonstrates **Hierarchical Clustering**, an unsupervised machine learning technique, using the classic **Iris Flower Dataset**. The goal is to group flowers based solely on their physical measurements and observe whether natural clusters emerge â€” *without using labels*.

This repository is designed to be:

* âœ… Beginner-friendly
* âœ… Interview-ready
* âœ… Concept-focused with visual intuition

---

## ğŸ¯ What You Will Learn

* How hierarchical clustering works internally
* How to **read and interpret a dendrogram**
* How to **choose the number of clusters visually**
* Differences between **Hierarchical Clustering vs K-Means**
* How to **explain this project confidently in interviews**

---

## ğŸ“‚ Dataset Information

**Dataset Name:** Iris Flower Dataset
**Source:** Kaggle

### ğŸ”¹ Features Used

| Column Name | Description            |
| ----------- | ---------------------- |
| SepalLength | Sepal length of flower |
| SepalWidth  | Sepal width of flower  |
| PetalLength | Petal length of flower |
| PetalWidth  | Petal width of flower  |

ğŸš« **Species column is NOT used** (unsupervised learning)

---

## ğŸ§  Problem Statement

> Group flowers based only on their physical measurements and analyze whether meaningful natural clusters appear.

---

## ğŸ› ï¸ Step-by-Step Implementation

### ğŸ”¹ Step 0: Import Required Libraries

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

---

### ğŸ”¹ Step 1: Load the Dataset

```python
df = pd.read_csv("Iris.csv")
df.head()
```

---

### ğŸ”¹ Step 2: Drop the Label Column

```python
X = df.drop(columns=["Species"])
```

ğŸ§  **Why?**
Hierarchical clustering is an **unsupervised learning** algorithm â€” no target variable is used.

---

### ğŸ”¹ Step 3: Feature Scaling (Mandatory)

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

ğŸ“Œ Distance-based algorithms require scaling to prevent bias toward larger features.

---

### ğŸ”¹ Step 4: Create the Dendrogram (Key Step)

```python
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_scaled, method='ward')

plt.figure(figsize=(10, 6))
dendrogram(linked)
plt.title("Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()
```

### ğŸ§  How to Read a Dendrogram

* Vertical lines â†’ clusters merging
* Large vertical gap â†’ optimal place to cut
* Horizontal cut â†’ decides number of clusters

ğŸ“Œ For Iris Dataset â†’ **3 clusters** is optimal

---

### ğŸ”¹ Step 5: Apply Hierarchical Clustering

```python
from sklearn.cluster import AgglomerativeClustering

model = AgglomerativeClustering(
    n_clusters=3,
    linkage='ward'
)

labels = model.fit_predict(X_scaled)
```

---

### ğŸ”¹ Step 6: Add Cluster Labels to Dataset

```python
df["Cluster"] = labels
df.head()
```

---

### ğŸ”¹ Step 7: Visualize the Clusters

```python
plt.figure(figsize=(8,6))
sns.scatterplot(
    x=df["PetalLength"],
    y=df["PetalWidth"],
    hue=labels,
    palette="viridis"
)
plt.title("Hierarchical Clustering of Iris Flowers")
plt.show()
```

---

### ğŸ”¹ Step 8: Cluster Interpretation

| Cluster | Interpretation       |
| ------- | -------------------- |
| 0       | Small petal flowers  |
| 1       | Medium petal flowers |
| 2       | Large petal flowers  |

ğŸ“Œ Even without labels, **natural grouping is clearly visible**.

---

## ğŸš€ Key Takeaways

* Hierarchical clustering does **not require predefined k**
* Dendrogram provides **strong visual intuition**
* Works well for **small to medium datasets**
* Excellent for **exploratory data analysis**

---
\

â­ If you found this helpful, consider starring the repo and connecting with me!

Happy Learning & Building ğŸš€
